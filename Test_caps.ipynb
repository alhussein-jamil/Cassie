{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "import functions as f \n",
    "obs_shape = (4,)\n",
    "num_steps = 5\n",
    "obs = np.random.rand(num_steps, *obs_shape)\n",
    "actions = np.random.rand(num_steps, 1)\n",
    "import torch\n",
    "train_batch = SampleBatch({\"obs\": torch.tensor(obs), \"actions\": torch.tensor(actions)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7717, 0.7555, 0.4162, 0.8083],\n",
       "        [0.6699, 0.3030, 0.8442, 0.6400],\n",
       "        [0.4124, 0.3183, 0.8200, 0.7787],\n",
       "        [0.2755, 0.8202, 0.6806, 0.6561],\n",
       "        [0.7704, 0.8374, 0.4464, 0.8358]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[\"obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random torch model that takes the observations and outputs the logits\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import torch.distributions.normal as normal\n",
    "import torch.distributions.multivariate_normal as multivariate_normal\n",
    "import torch.distributions.kl as kl\n",
    "import torch.distributions.bernoulli as bernoulli\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, obs_shape, action_shape):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_shape[0], 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_shape[0])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model((5,4), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss1(train_batch):\n",
    "    # get the observations and actions\n",
    "    obs, actions = train_batch[\"obs\"], train_batch[\"actions\"]\n",
    "    loss= 0 \n",
    "    # get the logits and the state of the model\n",
    "    logits, _ = model(obs)\n",
    "    \n",
    "    # calculate the mean of L_T and L_S over the training batch\n",
    "    L_S = 0\n",
    "\n",
    "\n",
    "    #get a bunch of normal distribution around \n",
    "    dist = torch.distributions.Normal(obs, 0.1 )\n",
    "\n",
    "    around_obs = dist.sample()\n",
    "\n",
    "    logits_around, _ = model( around_obs)\n",
    "\n",
    "\n",
    "\n",
    "    L_S = 0\n",
    "    L_T = 0\n",
    "\n",
    "    for i in range (len(train_batch[\"actions\"])):\n",
    "\n",
    "\n",
    "        # get the loss of the state around the observations\n",
    "        L_S += torch.mean(abs(logits[i]-logits_around[i]))\n",
    "\n",
    "        # get the loss of the actions around the observations\n",
    "        if(i>0):\n",
    "            L_T +=  f.action_dist(actions[i],actions[i-1])\n",
    "        \n",
    "    L_S = L_S / len(train_batch[\"actions\"])\n",
    "    L_T = L_T / len(train_batch[\"actions\"])\n",
    "    \n",
    "    # add the loss of the state around the observations to the loss\n",
    "    loss +=  L_S\n",
    "    loss += L_T\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def loss2(train_batch: SampleBatch):\n",
    "    loss = 0 \n",
    "    # get the observations and actions\n",
    "    obs, actions = train_batch[\"obs\"], train_batch[\"actions\"]\n",
    "    \n",
    "    # get the logits and the state of the model\n",
    "    logits, _ = model(obs)\n",
    "    \n",
    "    # calculate the mean of L_T and L_S over the training batch\n",
    "    L_S = 0\n",
    "\n",
    "    # get a bunch of normal distribution around \n",
    "    dist = torch.distributions.Normal(obs, 0.1 )\n",
    "    around_obs = dist.sample()\n",
    "\n",
    "    logits_around, _ = model(around_obs)\n",
    "\n",
    "    # get the loss of the state around the observations\n",
    "    L_S = torch.mean(torch.abs(logits - logits_around))\n",
    "\n",
    "    # get the loss of the actions around the observations\n",
    "    L_T = f.action_dist(actions[1:], actions[:-1]).mean()\n",
    "\n",
    "    # add the loss of the state around the observations to the loss\n",
    "    loss += L_S\n",
    "    loss += L_T\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "#make random a and b \n",
    "a = torch.randn((4,10))\n",
    "b = torch.randn((4,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants as c \n",
    "act_ranges = torch.tensor(list(c.actuator_ranges.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.0000,  9.0000, 24.4000, 24.4000,  1.8000,  9.0000,  9.0000, 24.4000,\n",
       "        24.4000,  1.8000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_ranges[:,1]-act_ranges[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5389,  2.1878, -0.7917, -2.2485, -2.1279,  1.0671,  0.2755, -0.4149,\n",
      "          0.1187,  0.7629],\n",
      "        [-2.1978, -0.8232, -0.4930, -0.2439,  1.6287, -3.3434, -1.7422, -2.2262,\n",
      "          1.6929, -0.1549],\n",
      "        [-0.6918, -0.1134,  1.5816,  1.0610,  0.1303, -0.2547,  1.5620,  0.8637,\n",
      "         -1.3862, -0.3678],\n",
      "        [ 0.3428, -0.3592, -1.6619,  2.1461, -3.0107, -0.7946, -2.0887, -0.1350,\n",
      "          1.3064, -2.1819]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.2902, 1.0408, 0.3075, 2.0849])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functions as f \n",
    "f.action_dist(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#compare the two losses\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss1(train_batch) \u001b[39m-\u001b[39m loss2(train_batch)\n",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m, in \u001b[0;36mloss1\u001b[0;34m(train_batch)\u001b[0m\n\u001b[1;32m      4\u001b[0m loss\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m      5\u001b[0m \u001b[39m# get the logits and the state of the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m logits, _ \u001b[39m=\u001b[39m model(obs)\n\u001b[1;32m      8\u001b[0m \u001b[39m# calculate the mean of L_T and L_S over the training batch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m L_S \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/cassie/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 21\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 21\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     23\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/cassie/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/cassie/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "#compare the two losses\n",
    "loss1(train_batch) - loss2(train_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cassie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
