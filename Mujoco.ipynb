{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, act_dim)\n",
    "        self.logstd = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        mean = self.fc3(x)\n",
    "        std = torch.exp(self.logstd)\n",
    "        return mean, std\n",
    "\n",
    "# Defining the value network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ajvendetta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# The device to use\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Creating the environment and getting the observation and action dimensions\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(env_name)\n\u001b[0;32m     18\u001b[0m obs_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m act_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ajvendetta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m     render_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs)\n\u001b[0;32m    641\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    642\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    643\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    644\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[0;32m    645\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Ajvendetta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\mujoco\\hopper.py:20\u001b[0m, in \u001b[0;36mHopperEnv.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     19\u001b[0m     observation_space \u001b[39m=\u001b[39m Box(low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, high\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39minf, shape\u001b[39m=\u001b[39m(\u001b[39m11\u001b[39m,), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m---> 20\u001b[0m     MuJocoPyEnv\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     21\u001b[0m         \u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mhopper.xml\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m4\u001b[39;49m, observation_space\u001b[39m=\u001b[39;49mobservation_space, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m     utils\u001b[39m.\u001b[39mEzPickle\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ajvendetta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\mujoco\\mujoco_env.py:186\u001b[0m, in \u001b[0;36mMuJocoPyEnv.__init__\u001b[1;34m(self, model_path, frame_skip, observation_space, render_mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    176\u001b[0m     model_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     camera_name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    184\u001b[0m ):\n\u001b[0;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m MUJOCO_PY_IMPORT_ERROR \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mDependencyNotInstalled(\n\u001b[0;32m    187\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mMUJOCO_PY_IMPORT_ERROR\u001b[39m}\u001b[39;00m\u001b[39m. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         )\n\u001b[0;32m    190\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis version of the mujoco environments depends \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mon the mujoco-py bindings, which are no longer maintained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou are trying to precisely replicate previous works).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    198\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    199\u001b[0m         model_path,\n\u001b[0;32m    200\u001b[0m         frame_skip,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m         camera_name,\n\u001b[0;32m    207\u001b[0m     )\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
     ]
    }
   ],
   "source": [
    "# Defining some hyperparameters\n",
    "env_name = \"Hopper-v2\" # The mujoco environment name\n",
    "num_workers = 8 # The number of parallel workers\n",
    "num_steps = 2048 # The number of steps per worker per epoch\n",
    "num_epochs = 500 # The number of training epochs\n",
    "gamma = 0.99 # The discount factor\n",
    "lamda = 0.95 # The GAE parameter\n",
    "clip_ratio = 0.2 # The PPO clip ratio\n",
    "pi_lr = 3e-4 # The policy learning rate\n",
    "vf_lr = 1e-3 # The value function learning rate\n",
    "train_pi_iters = 80 # The number of policy gradient steps per epoch\n",
    "train_v_iters = 80 # The number of value function steps per epoch\n",
    "target_kl = 0.01 # The target KL divergence\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # The device to use\n",
    "\n",
    "# Creating the environment and getting the observation and action dimensions\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# Creating the policy and value networks and their optimizers\n",
    "pi_net = PolicyNetwork(obs_dim, act_dim).to(device)\n",
    "vf_net = ValueNetwork(obs_dim).to(device)\n",
    "pi_optimizer = optim.Adam(pi_net.parameters(), lr=pi_lr)\n",
    "vf_optimizer = optim.Adam(vf_net.parameters(), lr=vf_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to collect trajectories from multiple workers using the current policy network\n",
    "def collect_trajectories(net):\n",
    "    states_list, actions_list, rewards_list, dones_list, log_probs_list, values_list = [], [], [], [], [], []\n",
    "    global_steps_list = []\n",
    "    # Creating a list of environments for each worker\n",
    "    envs = [gym.make(env_name) for _ in range(num_workers)]\n",
    "    # Initializing the states for each worker\n",
    "    states = torch.tensor([env.reset() for env in envs], dtype=torch.float32).to(device)\n",
    "    # Looping until we collect enough steps\n",
    "    global_steps = 0\n",
    "    while global_steps < num_steps:\n",
    "        # Getting the actions and log probabilities from the policy network\n",
    "        with torch.no_grad():\n",
    "            actions, log_probs = net.get_action(states)\n",
    "            values = vf_net(states).squeeze(-1)\n",
    "        # Taking a step in each environment\n",
    "        next_states, rewards, dones, _ = zip(*[env.step(action.cpu().numpy()) for env, action in zip(envs, actions)])\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "        # Storing the trajectories\n",
    "        states_list.append(states)\n",
    "        actions_list.append(actions)\n",
    "        rewards_list.append(rewards)\n",
    "        dones_list.append(dones)\n",
    "        log_probs_list.append(log_probs)\n",
    "        values_list.append(values)\n",
    "        # Updating the states and the global steps\n",
    "        states = next_states\n",
    "        global_steps += num_workers\n",
    "        global_steps_list.append(global_steps)\n",
    "    # Getting the last value estimate for each worker\n",
    "    with torch.no_grad():\n",
    "        last_values = vf_net(states).squeeze(-1)\n",
    "    values_list.append(last_values)\n",
    "    # Concatenating the trajectories\n",
    "    states = torch.cat(states_list, dim=0)\n",
    "    actions = torch.cat(actions_list, dim=0)\n",
    "    rewards = torch.cat(rewards_list, dim=0)\n",
    "    dones = torch.cat(dones_list, dim=0)\n",
    "    log_probs = torch.cat(log_probs_list, dim=0)\n",
    "    values = torch.cat(values_list, dim=0)\n",
    "    # Computing the advantage estimates and the returns\n",
    "    advantages = compute_advantage(rewards, values, dones)\n",
    "    returns = compute_return(rewards, values, dones)\n",
    "    # Normalizing the advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    # Closing the environments\n",
    "    for env in envs:\n",
    "        env.close()\n",
    "    return states, actions, log_probs, returns, advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to compute the log probability of an action given a state and a policy network\n",
    "def compute_log_prob(state, action, net):\n",
    "    mean, std = net(state)\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    log_prob = dist.log_prob(action).sum(axis=-1)\n",
    "    return log_prob\n",
    "\n",
    "# Defining a function to compute the advantage estimates using Generalized Advantage Estimation (GAE)\n",
    "def compute_advantage(rewards, values, dones):\n",
    "    advantages = torch.zeros_like(rewards).to(device)\n",
    "    last_advantage = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t+1] * (1-dones[t]) - values[t]\n",
    "        advantages[t] = delta + gamma * lamda * (1-dones[t]) * last_advantage\n",
    "        last_advantage = advantages[t]\n",
    "    return advantages\n",
    "\n",
    "# Defining a function to compute the discounted returns\n",
    "def compute_return(rewards, values, dones):\n",
    "    returns = torch.zeros_like(rewards).to(device)\n",
    "    last_return = values[-1]\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        returns[t] = rewards[t] + gamma * last_return * (1-dones[t])\n",
    "        last_return = returns[t]\n",
    "    return returns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cfca53093f074d58bafa9b2466c15e18e4cd4492afc4452f2f04d8da8850d19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
